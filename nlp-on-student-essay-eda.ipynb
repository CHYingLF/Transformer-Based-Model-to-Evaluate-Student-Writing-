{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"* This is an Exploratory Data Analysis for the Kaggle Competition 'Feedback Prize - Evaluating Student Writing'\n* Competition Website: https://www.kaggle.com/c/feedback-prize-2021/overview\n* Notebook Reference: https://www.kaggle.com/erikbruin/nlp-on-student-writing-eda","metadata":{}},{"cell_type":"markdown","source":"# 1. Import Libraries","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom glob import glob #In Python, the glob module is used to retrieve files/pathnames matching a specified pattern.\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport matplotlib.style as style\nstyle.use(\"fivethirtyeight\")\nfrom matplotlib.ticker import FuncFormatter\nfrom nltk.corpus import stopwords\nfrom tqdm.notebook import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')\nimport spacy\nfrom sklearn.feature_extraction.text import CountVectorizer\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-22T17:30:09.595363Z","iopub.execute_input":"2022-02-22T17:30:09.595738Z","iopub.status.idle":"2022-02-22T17:30:21.004737Z","shell.execute_reply.started":"2022-02-22T17:30:09.595647Z","shell.execute_reply":"2022-02-22T17:30:21.003879Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# 2. Read Data","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('../input/feedback-prize-2021/train.csv')\n#train.describe()\n#train.info()\ntrain[['discourse_id','discourse_start','discourse_end']] = train[['discourse_id','discourse_start','discourse_end']].astype(int)\n\nsample_submission = pd.read_csv('../input/feedback-prize-2021/sample_submission.csv')\n\n#The glob module finds all the pathnames matching a specified pattern according to the rules used by the Unix shell\ntrain_text = glob('../input/feedback-prize-2021/train/*.txt')\ntest_text = glob('../input/feedback-prize-2021/test/*.txt')","metadata":{"execution":{"iopub.status.busy":"2022-02-22T17:30:21.006214Z","iopub.execute_input":"2022-02-22T17:30:21.006458Z","iopub.status.idle":"2022-02-22T17:30:24.025768Z","shell.execute_reply.started":"2022-02-22T17:30:21.006429Z","shell.execute_reply":"2022-02-22T17:30:24.024715Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"! cat ../input/feedback-prize-2021/train/0000D23A521A.txt","metadata":{"execution":{"iopub.status.busy":"2022-02-22T17:30:24.029263Z","iopub.execute_input":"2022-02-22T17:30:24.029538Z","iopub.status.idle":"2022-02-22T17:30:24.806062Z","shell.execute_reply.started":"2022-02-22T17:30:24.029509Z","shell.execute_reply":"2022-02-22T17:30:24.805132Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"print(len(train_text))\ntrain['id'].nunique()","metadata":{"execution":{"iopub.status.busy":"2022-02-22T17:30:24.809996Z","iopub.execute_input":"2022-02-22T17:30:24.810258Z","iopub.status.idle":"2022-02-22T17:30:24.837206Z","shell.execute_reply.started":"2022-02-22T17:30:24.810230Z","shell.execute_reply":"2022-02-22T17:30:24.836400Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"print(len(train_text))\ntrain.query('id == \"0000D23A521A\"')","metadata":{"execution":{"iopub.status.busy":"2022-02-22T17:30:24.839535Z","iopub.execute_input":"2022-02-22T17:30:24.840192Z","iopub.status.idle":"2022-02-22T17:30:24.879768Z","shell.execute_reply.started":"2022-02-22T17:30:24.840147Z","shell.execute_reply":"2022-02-22T17:30:24.878878Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Length of Discourse Type and Predictionstring\n\n* length of discourse_text and predictionstring should be same\n* But there are quite many are not same, difference in 1, this is because of the token calculation","metadata":{}},{"cell_type":"code","source":"train['discourse_len'] = train['discourse_text'].apply(lambda x: len(x.split()))\n\ntrain[\"pred_len\"] = train[\"predictionstring\"].apply(lambda x: len(x.split()))\n\n\ncols_to_display = ['discourse_id', 'discourse_text', 'discourse_type','predictionstring', 'discourse_len', 'pred_len']\ntrain[cols_to_display].head()","metadata":{"execution":{"iopub.status.busy":"2022-02-22T17:30:24.881533Z","iopub.execute_input":"2022-02-22T17:30:24.882003Z","iopub.status.idle":"2022-02-22T17:30:25.799214Z","shell.execute_reply.started":"2022-02-22T17:30:24.881961Z","shell.execute_reply":"2022-02-22T17:30:25.798315Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"train.query('discourse_len != pred_len')","metadata":{"execution":{"iopub.status.busy":"2022-02-22T17:30:25.801005Z","iopub.execute_input":"2022-02-22T17:30:25.801507Z","iopub.status.idle":"2022-02-22T17:30:25.828093Z","shell.execute_reply.started":"2022-02-22T17:30:25.801463Z","shell.execute_reply":"2022-02-22T17:30:25.827501Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# 3. Frequency of discourse type and average length of the text","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize = (12,8))\n\nax1 = fig.add_subplot(211)\ntrain.groupby('discourse_type')['discourse_type'].count().sort_values().plot(kind = 'barh')\nax1.get_xaxis().set_major_formatter(FuncFormatter(lambda x, p: format(int(x),','))) #add thousands separator\nax1.set_title(\"Frequency of Discourse Type in all Essays\")\nax1.set_xlabel(\"Frequency\", fontsize = 10)\nax1.set_ylabel(\"\")\n\nax2 = fig.add_subplot(212)\ntrain.groupby('discourse_type')['discourse_len'].mean().sort_values().plot(kind = 'barh')\n#ax2.get_xaxis().set_major_formatter(FuncFormatter(lambda x, p: format(int(x),','))) #add thousands separator\nax2.set_title(\"Average number of words verses discourse type\")\nax2.set_xlabel(\"Average number of words\", fontsize = 10)\nax2.set_ylabel(\"\")\n\nplt.tight_layout(pad=2)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-22T17:30:25.829061Z","iopub.execute_input":"2022-02-22T17:30:25.829794Z","iopub.status.idle":"2022-02-22T17:30:26.366657Z","shell.execute_reply.started":"2022-02-22T17:30:25.829759Z","shell.execute_reply":"2022-02-22T17:30:26.365971Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# Percent of the discourse type present in essays","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize = (12,8))\nave_per_essay = train['discourse_type_num'].value_counts(ascending = True).rename_axis('discourse_type_num').reset_index(name='count')\n# keep the original index column and rename it to be 'count'\n\nave_per_essay['perc'] = round((ave_per_essay['count'] / train['id'].nunique()),3)\nave_per_essay = ave_per_essay.set_index('discourse_type_num')\nax = ave_per_essay.query('perc > 0.03')['perc'].plot(kind='barh')\nax.set_title(\"discourse_type_num: Percent present in essays\", fontsize=20, fontweight = 'bold')\nax.bar_label(ax.containers[0],label_type='edge') #label column value\nax.set_xlabel('Percent')\nax.set_ylabel('')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-02-22T17:30:26.367648Z","iopub.execute_input":"2022-02-22T17:30:26.367997Z","iopub.status.idle":"2022-02-22T17:30:26.807462Z","shell.execute_reply.started":"2022-02-22T17:30:26.367967Z","shell.execute_reply":"2022-02-22T17:30:26.806491Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# Average positions of the discourse start and end","metadata":{}},{"cell_type":"code","source":"#Use pandas default plot to plot\ndata = train.groupby('discourse_type')[['discourse_end','discourse_start']].mean().reset_index().sort_values(by = 'discourse_start', ascending = False)\nax=data.plot(x = 'discourse_type',\n          kind = 'barh',\n         stacked=False,\n         title = 'Average start and end position absolute',\n         figsize =(12,4))\n\nfor container in ax.containers:\n    ax.bar_label(container, fontsize=10)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-22T17:30:26.810084Z","iopub.execute_input":"2022-02-22T17:30:26.810337Z","iopub.status.idle":"2022-02-22T17:30:27.158208Z","shell.execute_reply.started":"2022-02-22T17:30:26.810306Z","shell.execute_reply":"2022-02-22T17:30:27.157389Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# 4. Investigation the gaps between annotations (text not used as discourse_text)","metadata":{}},{"cell_type":"markdown","source":"* There are quite many chunks that are not classified","metadata":{}},{"cell_type":"code","source":"len_dict = {}\nword_dict = {}\nfor t in tqdm(train_text):\n    with open(t, \"r\") as txt_file:\n        myid = t.split(\"/\")[-1].replace(\".txt\",\"\")\n        data = txt_file.read()\n        mylen = len(data.strip()) #character len of the essay\n        myword = len(data.split())\n        len_dict[myid] = mylen\n        word_dict[myid] = myword\n        \ntrain['essay_len'] = train['id'].map(len_dict)\ntrain['essay_words'] = train['id'].map(word_dict) \n    ","metadata":{"execution":{"iopub.status.busy":"2022-02-22T17:30:27.159503Z","iopub.execute_input":"2022-02-22T17:30:27.160279Z","iopub.status.idle":"2022-02-22T17:32:03.213794Z","shell.execute_reply.started":"2022-02-22T17:30:27.160231Z","shell.execute_reply":"2022-02-22T17:32:03.212876Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"* Calculate the gaps","metadata":{}},{"cell_type":"code","source":"#initialize the column\ntrain['gap_length'] = np.nan\n\n#set the first one\ntrain.loc[0, 'gap_length'] = 7\n\n#loop over rest\nfor i in tqdm(range(1, len(train))):\n    #gap if difference is not 1 within an essay\n    if ((train.loc[i, 'id'] == train.loc[i-1,\"id\"])\\\n       and (train.loc[i, \"discourse_start\"] - train.loc[i-1,'discourse_end']>1)):\n        train.loc[i, 'gap_length'] = train.loc[i, 'discourse_start']-train.loc[i-1,\"discourse_end\"]-2\n        \n    elif ((train.loc[i,'id'] != train.loc[i-1,'id'])\\\n         and (train.loc[i,'discourse_start'] != 0)):\n        train.loc[i, 'gap_length'] = train.loc[i, 'discourse_start']-1\n    \n    \n        \n","metadata":{"execution":{"iopub.status.busy":"2022-02-22T17:32:03.215225Z","iopub.execute_input":"2022-02-22T17:32:03.215558Z","iopub.status.idle":"2022-02-22T17:32:36.014577Z","shell.execute_reply.started":"2022-02-22T17:32:03.215517Z","shell.execute_reply":"2022-02-22T17:32:36.013700Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"* text after the last discourse","metadata":{}},{"cell_type":"code","source":"last_ones = train.drop_duplicates(subset = 'id', keep = 'last')\n\n#np.where(condition, x, y), choose x if condition else y\nlast_ones['gap_end_length'] = np.where((last_ones.discourse_end < last_ones.essay_len),\\\n                                       (last_ones.essay_len - last_ones.discourse_end),\\\n                                       np.nan)\n\ncols_to_merge = ['id','discourse_id','gap_end_length']\ntrain = train.merge(last_ones[cols_to_merge], on=['id','discourse_id'], how = 'left')","metadata":{"execution":{"iopub.status.busy":"2022-02-22T17:32:36.015906Z","iopub.execute_input":"2022-02-22T17:32:36.016129Z","iopub.status.idle":"2022-02-22T17:32:36.169453Z","shell.execute_reply.started":"2022-02-22T17:32:36.016102Z","shell.execute_reply":"2022-02-22T17:32:36.168439Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"col_to_show = ['id','discourse_start', 'discourse_end','discourse_type','essay_len','gap_length', 'gap_end_length']\ntrain[col_to_show].query('id == \"4C471936CD75\"')","metadata":{"execution":{"iopub.status.busy":"2022-02-22T17:32:36.170665Z","iopub.execute_input":"2022-02-22T17:32:36.170911Z","iopub.status.idle":"2022-02-22T17:32:36.233258Z","shell.execute_reply.started":"2022-02-22T17:32:36.170882Z","shell.execute_reply":"2022-02-22T17:32:36.232423Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"print(f\"Besides the {len(train)} discourse texts, there are {len(train.query('gap_length.notna()', engine='python'))+len(train.query('gap_end_length.notna()', engine = 'python'))} pieces of text not classified. \")","metadata":{"execution":{"iopub.status.busy":"2022-02-22T17:32:36.234805Z","iopub.execute_input":"2022-02-22T17:32:36.235094Z","iopub.status.idle":"2022-02-22T17:32:36.260152Z","shell.execute_reply.started":"2022-02-22T17:32:36.235054Z","shell.execute_reply":"2022-02-22T17:32:36.259489Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# huge gaps\ntrain.sort_values(by = 'gap_length', ascending = False)[col_to_show]","metadata":{"execution":{"iopub.status.busy":"2022-02-22T17:32:36.263065Z","iopub.execute_input":"2022-02-22T17:32:36.263559Z","iopub.status.idle":"2022-02-22T17:32:36.316124Z","shell.execute_reply.started":"2022-02-22T17:32:36.263509Z","shell.execute_reply":"2022-02-22T17:32:36.315182Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"train.sort_values(by='gap_end_length', ascending = False)[col_to_show]","metadata":{"execution":{"iopub.status.busy":"2022-02-22T17:32:36.317240Z","iopub.execute_input":"2022-02-22T17:32:36.317493Z","iopub.status.idle":"2022-02-22T17:32:36.366093Z","shell.execute_reply.started":"2022-02-22T17:32:36.317462Z","shell.execute_reply":"2022-02-22T17:32:36.365423Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"* Below, you can see a histogram of the length of all gaps with the outliers taken out (all gaps longer than 300 characters).","metadata":{"execution":{"iopub.status.busy":"2022-02-18T22:36:10.514474Z","iopub.execute_input":"2022-02-18T22:36:10.514788Z","iopub.status.idle":"2022-02-18T22:36:10.52087Z","shell.execute_reply.started":"2022-02-18T22:36:10.514758Z","shell.execute_reply":"2022-02-18T22:36:10.519589Z"}}},{"cell_type":"code","source":"all_gaps = (train.gap_length[~train.gap_length.isna()]).append((train.gap_end_length[~train.gap_end_length.isna()]), ignore_index = True)\n#filter out outliers\nall_gaps = all_gaps[all_gaps<300]\n\nfig = plt.figure(figsize = (12,6))\nall_gaps.plot(kind = 'hist', bins=100)\nplt.title(\"Histogram of gap length (gaps up to 300 characters only)\")\nplt.xticks(rotation=0)\nplt.xlabel(\"Length of gaps in characters\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-22T17:32:36.367727Z","iopub.execute_input":"2022-02-22T17:32:36.367953Z","iopub.status.idle":"2022-02-22T17:32:36.759960Z","shell.execute_reply.started":"2022-02-22T17:32:36.367925Z","shell.execute_reply":"2022-02-22T17:32:36.759023Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"# Essays with large percentage of text not classified\n\n* More than 80 percent of the text not classified","metadata":{}},{"cell_type":"code","source":"total_gaps = train.groupby('id').agg({'essay_len':'first',\\\n                                       'gap_length':'sum',\\\n                                       'gap_end_length':'sum'})\ntotal_gaps['perc_not_classified'] = round(((total_gaps.gap_length + total_gaps.gap_end_length)/total_gaps.essay_len),2)\ntotal_gaps.sort_values('perc_not_classified', ascending = False).query(\"perc_not_classified > 0.8\")","metadata":{"execution":{"iopub.status.busy":"2022-02-22T19:12:54.380049Z","iopub.execute_input":"2022-02-22T19:12:54.380400Z","iopub.status.idle":"2022-02-22T19:12:54.456072Z","shell.execute_reply.started":"2022-02-22T19:12:54.380358Z","shell.execute_reply":"2022-02-22T19:12:54.454890Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"markdown","source":"# 5. Color printing essays including the gaps","metadata":{}},{"cell_type":"code","source":"def add_gap_rows(essay):\n    cols_to_keep = ['discourse_start', 'discourse_end', 'discourse_type', 'gap_length', 'gap_end_length']\n    df_essay = train.query('id == @essay')[cols_to_keep].reset_index(drop = True)\n\n    #index new row\n    insert_row = len(df_essay)\n   \n    for i in range(1, len(df_essay)):          \n        if df_essay.loc[i,\"gap_length\"] >0:\n            if i == 0:\n                start = 0 #as there is no i-1 for first row\n                end = df_essay.loc[0, 'discourse_start'] -1\n                disc_type = \"Nothing\"\n                gap_end = np.nan\n                gap = np.nan\n                df_essay.loc[insert_row] = [start, end, disc_type, gap, gap_end]\n                insert_row += 1\n            else:\n                start = df_essay.loc[i-1, \"discourse_end\"] + 1\n                end = df_essay.loc[i, 'discourse_start'] -1\n                disc_type = \"Nothing\"\n                gap_end = np.nan\n                gap = np.nan\n                df_essay.loc[insert_row] = [start, end, disc_type, gap, gap_end]\n                insert_row += 1\n\n    df_essay = df_essay.sort_values(by = \"discourse_start\").reset_index(drop=True)\n\n    #add gap at end\n    if df_essay.loc[(len(df_essay)-1),'gap_end_length'] > 0:\n        start = df_essay.loc[(len(df_essay)-1), \"discourse_end\"] + 1\n        end = start + df_essay.loc[(len(df_essay)-1), 'gap_end_length']\n        disc_type = \"Nothing\"\n        gap_end = np.nan\n        gap = np.nan\n        df_essay.loc[insert_row] = [start, end, disc_type, gap, gap_end]\n        \n    return(df_essay)\n\nadd_gap_rows(\"129497C3E0FC\")","metadata":{"execution":{"iopub.status.busy":"2022-02-22T17:32:36.832509Z","iopub.execute_input":"2022-02-22T17:32:36.833001Z","iopub.status.idle":"2022-02-22T17:32:36.886147Z","shell.execute_reply.started":"2022-02-22T17:32:36.832953Z","shell.execute_reply":"2022-02-22T17:32:36.885288Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"def print_colored_essay(essay):\n    df_essay = add_gap_rows(essay)\n    #code from https://www.kaggle.com/odins0n/feedback-prize-eda, but adjusted to df_essay\n    essay_file = \"../input/feedback-prize-2021/train/\" + essay + \".txt\"\n\n    ents = []\n    for i, row in df_essay.iterrows():\n        ents.append({\n                        'start': int(row['discourse_start']), \n                         'end': int(row['discourse_end']), \n                         'label': row['discourse_type']\n                    })\n\n    with open(essay_file, 'r') as file: data = file.read()\n\n    doc2 = {\n        \"text\": data,\n        \"ents\": ents,\n    }\n\n    colors = {'Lead': '#EE11D0','Position': '#AB4DE1','Claim': '#1EDE71','Evidence': '#33FAFA','Counterclaim': '#4253C1','Concluding Statement': 'yellow','Rebuttal': 'red'}\n    options = {\"ents\": df_essay.discourse_type.unique().tolist(), \"colors\": colors}\n    spacy.displacy.render(doc2, style=\"ent\", options=options, manual=True, jupyter=True);","metadata":{"execution":{"iopub.status.busy":"2022-02-22T17:32:36.887254Z","iopub.execute_input":"2022-02-22T17:32:36.887503Z","iopub.status.idle":"2022-02-22T17:32:36.898480Z","shell.execute_reply.started":"2022-02-22T17:32:36.887473Z","shell.execute_reply":"2022-02-22T17:32:36.897612Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"print_colored_essay(\"7330313ED3F0\")","metadata":{"execution":{"iopub.status.busy":"2022-02-22T17:32:36.899793Z","iopub.execute_input":"2022-02-22T17:32:36.900478Z","iopub.status.idle":"2022-02-22T17:32:36.935883Z","shell.execute_reply.started":"2022-02-22T17:32:36.900436Z","shell.execute_reply":"2022-02-22T17:32:36.934924Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"# 6. Most Used Words per Discourse Type","metadata":{}},{"cell_type":"code","source":"train['discourse_text'] = train['discourse_text'].str.lower()\n\n#get stopwords from nltk library\nstop_english = stopwords.words(\"english\")\nother_words_to_take_out = ['school', 'students', 'people', 'would', 'could', 'many']\nstop_english.extend(other_words_to_take_out)\n\n\n#put dataframe of Top-10 words in dict for all discourse types\ncounts_dict = {}\n\nfor dt in tqdm(train['discourse_type'].unique()):\n    df = train.query(\"discourse_type == @dt\") #dt has been assigned, so use @dt\n    text = df.discourse_text.apply(lambda x: x.split()).tolist() #convert each text into list, then combine all text list into one list. [[],[],[]]\n    text = [item for elem in text for item in elem] #Combine all the words\n    df1 = pd.Series(text).value_counts().to_frame().reset_index()\n    df1.columns = ['Word', 'Frequency']\n    df1 = df1[~df1.Word.isin(stop_english)].head(10)\n    #print(df1)\n    df1 = df1.set_index(\"Word\").sort_values(by = \"Frequency\", ascending = True)\n    #print(df1)\n    counts_dict[dt] = df1\n\nplt.figure(figsize=(15,12))\nplt.subplots_adjust(hspace=0.5)\nkeys = list(counts_dict.keys())\n\nfor n,key in enumerate(keys):\n    ax = plt.subplot(4,2,n+1)\n    ax.set_title(f\"Most used words in {key}\")\n    counts_dict[keys[n]].plot(ax=ax, kind = 'barh')\n    plt.ylabel(\"\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-22T17:32:36.937185Z","iopub.execute_input":"2022-02-22T17:32:36.937460Z","iopub.status.idle":"2022-02-22T17:32:42.140052Z","shell.execute_reply.started":"2022-02-22T17:32:36.937432Z","shell.execute_reply":"2022-02-22T17:32:42.139014Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"# Making n_grams for each discourse type","metadata":{}},{"cell_type":"code","source":"def get_n_grams(n_grams,top_n = 10):\n    df_words = pd.DataFrame() #empty dataframe\n    for dt in tqdm(train['discourse_type'].unique()):\n        df = train.query('discourse_type == @dt') #take out all dt type text, like Lead\n        texts = df['discourse_text'].tolist() #transform dataframe to list, [[],[],[]...]\n        vec = CountVectorizer(lowercase = True, stop_words='english', ngram_range = (n_grams, n_grams)).fit(texts) #Learn a vocabulary dictionary of all tokens in the raw documents.\n        bag_of_words = vec.transform(texts) #Transform documents to document-term matrix. Each text count words in global words vector\n        sum_words = bag_of_words.sum(axis = 0)\n        words_freq = [(word, sum_words[0,idx]) for word, idx in vec.vocabulary_.items()]\n        cvec_df = pd.DataFrame.from_records(words_freq, columns = ['words','counts']).sort_values(by=\"counts\", ascending = False) #from list to dataframe\n        cvec_df.insert(0, \"Discourse_type\", dt) #insert discourse_type column into dataframe\n        cvec_df = cvec_df.iloc[:top_n,:] # select top_n words\n        df_words = df_words.append(cvec_df) #append to global df_words\n    return df_words","metadata":{"execution":{"iopub.status.busy":"2022-02-22T18:25:50.236305Z","iopub.execute_input":"2022-02-22T18:25:50.236629Z","iopub.status.idle":"2022-02-22T18:25:50.245908Z","shell.execute_reply.started":"2022-02-22T18:25:50.236594Z","shell.execute_reply":"2022-02-22T18:25:50.244954Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"markdown","source":"# Bigram","metadata":{}},{"cell_type":"code","source":"bigrams = get_n_grams(2, 10)\nbigrams.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-22T18:26:22.342040Z","iopub.execute_input":"2022-02-22T18:26:22.342359Z","iopub.status.idle":"2022-02-22T18:26:46.851078Z","shell.execute_reply.started":"2022-02-22T18:26:22.342310Z","shell.execute_reply":"2022-02-22T18:26:46.850022Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"def plot_ngram(df, type = 'bigram'):\n    plt.figure(figsize = (15,12))\n    plt.subplots_adjust(hspace = 0.5)\n    \n    for n, dt in enumerate(df.Discourse_type.unique()):\n        ax = plt.subplot(4,2,n+1)\n        ax.set_title(f\"Most used words {type} in {dt}\")\n        #query the discourse_type, take 'words' and 'counts' column, let 'words' as index(when plot, index will be ylabel)\n        data = df.query('Discourse_type == @dt')[['words','counts']].set_index('words').sort_values(by = 'counts', ascending = True)\n        data.plot(ax=ax, kind = 'barh')\n        plt.ylabel(\"\") #index value will be as the label\n    plt.tight_layout() #Adjust the padding between and around subplots.\n    plt.show()\n    \nplot_ngram(bigrams) ","metadata":{"execution":{"iopub.status.busy":"2022-02-22T18:38:01.809264Z","iopub.execute_input":"2022-02-22T18:38:01.809590Z","iopub.status.idle":"2022-02-22T18:38:03.886610Z","shell.execute_reply.started":"2022-02-22T18:38:01.809555Z","shell.execute_reply":"2022-02-22T18:38:03.885589Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"markdown","source":"# Trigrams","metadata":{}},{"cell_type":"code","source":"trigrams = get_n_grams(n_grams = 3, top_n=10)\nplot_ngram(trigrams, type = \"trigrams\")","metadata":{"execution":{"iopub.status.busy":"2022-02-22T18:43:44.551784Z","iopub.execute_input":"2022-02-22T18:43:44.552116Z","iopub.status.idle":"2022-02-22T18:44:16.951544Z","shell.execute_reply.started":"2022-02-22T18:43:44.552081Z","shell.execute_reply":"2022-02-22T18:44:16.950627Z"},"trusted":true},"execution_count":63,"outputs":[]}]}